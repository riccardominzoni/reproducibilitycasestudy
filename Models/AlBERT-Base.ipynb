{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "Uai-7oFitcGC",
    "outputId": "77410c04-4a8b-4f4c-9cd0-6b6dcbcae87d"
   },
   "outputs": [],
   "source": [
    "# !pip install pytreebank\n",
    "# !pip install loguru\n",
    "# !pip install transformers\n",
    "# ! pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytreebank\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from transformers import AlbertTokenizer, AlbertConfig, AlbertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oX5RMK1Qs3K4",
    "outputId": "8e2db6ab-47c8-44da-94e0-d6e308224fc9"
   },
   "outputs": [],
   "source": [
    "\"\"\"This module defines a configurable SSTDataset class.\"\"\"\n",
    "\n",
    "\n",
    "logger.info(\"Loading the tokenizer\")\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "logger.info(\"Loading SST\")\n",
    "sst = pytreebank.load_sst()\n",
    "\n",
    "\n",
    "def rpad(array, n=70):\n",
    "    \"\"\"Right padding.\"\"\"\n",
    "    current_len = len(array)\n",
    "    if current_len > n:\n",
    "        return array[: n - 1]\n",
    "    extra = n - current_len\n",
    "    return array + ([0] * extra)\n",
    "\n",
    "\n",
    "def get_binary_label(label):\n",
    "    \"\"\"Convert fine-grained label to binary label.\"\"\"\n",
    "    if label < 2:\n",
    "        return 0\n",
    "    if label > 2:\n",
    "        return 1\n",
    "    raise ValueError(\"Invalid label\")\n",
    "\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    \"\"\"Configurable SST Dataset.\n",
    "    \n",
    "    Things we can configure:\n",
    "        - split (train / val / test)\n",
    "        - root / all nodes\n",
    "        - binary / fine-grained\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split=\"train\", root=True, binary=True):\n",
    "        \"\"\"Initializes the dataset with given configuration.\n",
    "\n",
    "        Args:\n",
    "            split: str\n",
    "                Dataset split, one of [train, val, test]\n",
    "            root: bool\n",
    "                If true, only use root nodes. Else, use all nodes.\n",
    "            binary: bool\n",
    "                If true, use binary labels. Else, use fine-grained.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading SST {split} set\")\n",
    "        self.sst = sst[split]\n",
    "\n",
    "        logger.info(\"Tokenizing\")\n",
    "        if root and binary:\n",
    "            self.data = [\n",
    "                (\n",
    "                    rpad(\n",
    "                        tokenizer.encode(\"[CLS] \" + tree.to_lines()[0] + \" [SEP]\"), n=66\n",
    "                    ),\n",
    "                    get_binary_label(tree.label),\n",
    "                )\n",
    "                for tree in self.sst\n",
    "                if tree.label != 2\n",
    "            ]\n",
    "        elif root and not binary:\n",
    "            self.data = [\n",
    "                (\n",
    "                    rpad(\n",
    "#                         tokenizer.encode(\"[CLS] \" + tree.to_lines()[0] + \" [SEP]\"), n=66\n",
    "                        tokenizer.encode(tree.to_lines()[0]), n=66\n",
    "                    ),\n",
    "                    tree.label,\n",
    "                )\n",
    "                for tree in self.sst\n",
    "            ]\n",
    "        elif not root and not binary:\n",
    "            self.data = [\n",
    "                (rpad(tokenizer.encode(\"[CLS] \" + line + \" [SEP]\"), n=66), label)\n",
    "                for tree in self.sst\n",
    "                for label, line in tree.to_labeled_lines()\n",
    "            ]\n",
    "        else:\n",
    "            self.data = [\n",
    "                (\n",
    "                    rpad(tokenizer.encode(\"[CLS] \" + line + \" [SEP]\"), n=66),\n",
    "                    get_binary_label(label),\n",
    "                )\n",
    "                for tree in self.sst\n",
    "                for label, line in tree.to_labeled_lines()\n",
    "                if label != 2\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.data[index]\n",
    "        X = torch.tensor(X)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNBkCHk7s3K9"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "### random seed \n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "\n",
    "def train_one_epoch(model, lossfn, optimizer, dataset, batch_size=8):\n",
    "    generator = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for batch, labels in tqdm(generator):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(batch, labels=labels)[:2]\n",
    "        err = lossfn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pred_labels = torch.argmax(logits, axis=1)\n",
    "        train_acc += (pred_labels == labels).sum().item()\n",
    "    train_loss /= len(dataset)\n",
    "    train_acc /= len(dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def evaluate_one_epoch(model, lossfn, optimizer, dataset, batch_size=8):\n",
    "    generator = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    model.eval()\n",
    "    loss, acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in tqdm(generator):\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            logits = model(batch)[0]\n",
    "            error = lossfn(logits, labels)\n",
    "            loss += error.item()\n",
    "            pred_labels = torch.argmax(logits, axis=1)\n",
    "            acc += (pred_labels == labels).sum().item()\n",
    "    loss /= len(dataset)\n",
    "    acc /= len(dataset)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    root=True,\n",
    "    binary=False,\n",
    "    bert=\"albert-base-v2\",\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    patience = 5,\n",
    "    save=False,\n",
    "):\n",
    "    trainset = SSTDataset(\"train\", root=root, binary=binary)\n",
    "    devset = SSTDataset(\"dev\", root=root, binary=binary)\n",
    "    testset = SSTDataset(\"test\", root=root, binary=binary)\n",
    "\n",
    "    # REMOVE BAD TRAINING DATA\n",
    "\n",
    "    for x in trainset.data:\n",
    "        if len(x[0]) != 66:\n",
    "            trainset.data.remove(x)\n",
    "\n",
    "    for x in devset.data:\n",
    "        if len(x[0]) != 66:\n",
    "            devset.data.remove(x)\n",
    "\n",
    "    for x in testset.data:\n",
    "        if len(x[0]) != 66:\n",
    "            testset.data.remove(x)\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    \n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    last_loss = 100\n",
    "    patience = patience\n",
    "    triggertimes = 0     \n",
    "        \n",
    "    config = AlbertConfig.from_pretrained(bert)\n",
    "    if not binary:\n",
    "        config.num_labels = 5\n",
    "    model = AlbertForSequenceClassification.from_pretrained(bert, config=config)\n",
    "#     switch to GPU if available\n",
    "    model = model.to(device)\n",
    "    \n",
    "    lossfn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, lossfn, optimizer, trainset, batch_size=batch_size\n",
    "        )\n",
    "        val_loss, val_acc = evaluate_one_epoch(\n",
    "            model, lossfn, optimizer, devset, batch_size=batch_size\n",
    "        )\n",
    "        test_loss, test_acc = evaluate_one_epoch(\n",
    "            model, lossfn, optimizer, testset, batch_size=batch_size\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        logger.info(f\"epoch={epoch}\")\n",
    "        logger.info(\n",
    "            f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, test_loss={test_loss:.4f}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}, test_acc={test_acc:.3f}\"\n",
    "        )\n",
    "        if save: \n",
    "            label = \"binary\" if binary else \"fine\"\n",
    "            nodes = \"root\" if root else \"all\"\n",
    "            torch.save(model, f\"{bert}__{nodes}__{label}__e{epoch}.pickle\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        current_loss = val_loss\n",
    "        if current_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            logger.info(f\"Trigger Times: {trigger_times}\")\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                logger.info(f\"Done with Early Stopping at epoch {epoch}!\")\n",
    "                return train_losses, val_losses, test_losses, train_accuracies, val_accuracies, test_accuracies, epoch\n",
    "\n",
    "        else:\n",
    "            logger.info('Trigger Times: 0')\n",
    "            trigger_times = 0\n",
    "\n",
    "        last_loss = current_loss\n",
    "\n",
    "\n",
    "    logger.success(\"Done!\")\n",
    "    return train_losses, val_losses, test_losses, train_accuracies, val_accuracies, test_accuracies, epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_type = \"albert-base-v2\"\n",
    "                                                                                    \n",
    "total_train_losses = []\n",
    "total_val_losses = []\n",
    "total_test_losses = []\n",
    "total_train_accuracies = []\n",
    "total_val_accuracies = []\n",
    "total_test_accuracies = []\n",
    "\n",
    "for i in range(1,6):\n",
    "#     set_seed(42)\n",
    "    train_losses, val_losses, test_losses, train_accuracies, val_accuracies, test_accuracies, epoch = train(root=True,\n",
    "                                                                                                            binary=False,\n",
    "                                                                                                            bert=bert_type,\n",
    "                                                                                                            epochs=30,\n",
    "                                                                                                            batch_size=8, \n",
    "                                                                                                            patience = 30,\n",
    "                                                                                                            save=True)\n",
    "    total_train_losses += train_losses\n",
    "    total_val_losses += val_losses\n",
    "    total_test_losses += test_losses\n",
    "    total_train_accuracies += train_accuracies\n",
    "    total_val_accuracies += val_accuracies\n",
    "    total_test_accuracies += test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(total_train_losses,total_val_losses,total_test_losses,\n",
    "                           total_train_accuracies,total_val_accuracies,total_test_accuracies)),\n",
    "                                              columns =['Train Loss', 'Val Loss', 'Test Loss',\n",
    "                                                        'Train Accuracy', 'Val Accuracy', 'Test Accuracy'])\n",
    "\n",
    "df.to_csv('ALBERT_BASE_5.csv')\n",
    "df_1, df_2, df_3, df_4, df_5 = np.array_split(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = range(epoch)\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(15,8))\n",
    "fig.suptitle('Horizontally stacked subplots of Losses and Accuracies')\n",
    "axs[0,0].plot(EPOCH, df_1['Train Loss'])\n",
    "axs[0,0].plot(EPOCH, df_2['Train Loss'])\n",
    "axs[0,0].plot(EPOCH, df_3['Train Loss'])\n",
    "axs[0,0].plot(EPOCH, df_4['Train Loss'])\n",
    "axs[0,0].plot(EPOCH, df_5['Train Loss'])\n",
    "axs[0,0].set_title(\"Train Loss\")\n",
    "axs[0,1].plot(EPOCH, df_1['Val Loss'])\n",
    "axs[0,1].plot(EPOCH, df_2['Val Loss'])\n",
    "axs[0,1].plot(EPOCH, df_3['Val Loss'])\n",
    "axs[0,1].plot(EPOCH, df_4['Val Loss'])\n",
    "axs[0,1].plot(EPOCH, df_5['Val Loss'])\n",
    "axs[0,1].set_title(\"Validation Loss\")\n",
    "axs[0,2].plot(EPOCH, df_1['Test Loss'])\n",
    "axs[0,2].plot(EPOCH, df_2['Test Loss'])\n",
    "axs[0,2].plot(EPOCH, df_3['Test Loss'])\n",
    "axs[0,2].plot(EPOCH, df_4['Test Loss'])\n",
    "axs[0,2].plot(EPOCH, df_5['Test Loss'])\n",
    "axs[0,2].set_title(\"Test Loss\")\n",
    "axs[1,0].plot(EPOCH, df_1['Train Accuracy'])\n",
    "axs[1,0].plot(EPOCH, df_2['Train Accuracy'])\n",
    "axs[1,0].plot(EPOCH, df_3['Train Accuracy'])\n",
    "axs[1,0].plot(EPOCH, df_4['Train Accuracy'])\n",
    "axs[1,0].plot(EPOCH, df_5['Train Accuracy'])\n",
    "axs[1,0].set_title(\"Train Accuracy\")\n",
    "axs[1,1].plot(EPOCH, df_1['Val Accuracy'])\n",
    "axs[1,1].plot(EPOCH, df_2['Val Accuracy'])\n",
    "axs[1,1].plot(EPOCH, df_3['Val Accuracy'])\n",
    "axs[1,1].plot(EPOCH, df_4['Val Accuracy'])\n",
    "axs[1,1].plot(EPOCH, df_5['Val Accuracy'])\n",
    "axs[1,1].set_title(\"Validation Accuracy\")\n",
    "axs[1,2].plot(EPOCH, df_1['Test Accuracy'])\n",
    "axs[1,2].plot(EPOCH, df_2['Test Accuracy'])\n",
    "axs[1,2].plot(EPOCH, df_3['Test Accuracy'])\n",
    "axs[1,2].plot(EPOCH, df_4['Test Accuracy'])\n",
    "axs[1,2].plot(EPOCH, df_5['Test Accuracy'])\n",
    "axs[1,2].set_title(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = pd.DataFrame()\n",
    "df_concat = pd.concat([df_1.reset_index().drop(['index'],axis=1),\n",
    "                       df_2.reset_index().drop(['index'],axis=1),\n",
    "                       df_3.reset_index().drop(['index'],axis=1),\n",
    "                       df_4.reset_index().drop(['index'],axis=1),\n",
    "                       df_5.reset_index().drop(['index'],axis=1)], axis= 1)\n",
    "df_mean['Train Loss'] = df_concat['Train Loss'].mean(axis=1)\n",
    "df_mean['Val Loss'] = df_concat['Val Loss'].mean(axis=1)\n",
    "df_mean['Test Loss'] = df_concat['Test Loss'].mean(axis=1)\n",
    "df_mean['Train Accuracy'] = df_concat['Train Accuracy'].mean(axis=1)\n",
    "df_mean['Val Accuracy'] = df_concat['Val Accuracy'].mean(axis=1)\n",
    "df_mean['Test Accuracy'] = df_concat['Test Accuracy'].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = range(epoch)\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(15,8))\n",
    "fig.suptitle('Horizontally stacked subplots of Losses and Accuracies')\n",
    "axs[0,0].plot(EPOCH, df_mean['Train Loss'])\n",
    "axs[0,0].set_title(\"Train Loss\")\n",
    "axs[0,1].plot(EPOCH, df_mean['Val Loss'])\n",
    "axs[0,1].set_title(\"Validation Loss\")\n",
    "axs[0,2].plot(EPOCH, df_mean['Test Loss'])\n",
    "axs[0,2].set_title(\"Test Loss\")\n",
    "axs[1,0].plot(EPOCH, df_mean['Train Accuracy'])\n",
    "axs[1,0].set_title(\"Train Accuracy\")\n",
    "axs[1,1].plot(EPOCH, df_mean['Val Accuracy'])\n",
    "axs[1,1].set_title(\"Validation Accuracy\")\n",
    "axs[1,2].plot(EPOCH, df_mean['Test Accuracy'])\n",
    "axs[1,2].set_title(\"Test Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "albert-xxl.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
